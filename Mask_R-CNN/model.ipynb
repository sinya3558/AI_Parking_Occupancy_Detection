{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import library, Set up seed and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from torchvision.ops import box_iou\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import jaccard_score\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working device : cpu\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'current working device : {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Dataset for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: https://drive.google.com/uc?id=1AbU0ghVIxP81SR8_L17wtzdZwSvZvRZl\n",
      "unzip:  cannot find or open seg_dataset.zip, seg_dataset.zip.zip or seg_dataset.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"seg_dataset\"):\n",
    "    # !pip install gdown\n",
    "    !gdown https://drive.google.com/uc?id=1AbU0ghVIxP81SR8_L17wtzdZwSvZvRZl\n",
    "    !unzip -qq seg_dataset.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a class for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class for custom dataset\n",
    "class SegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Segmentation Dataset Class\n",
    "    - images_dir: literally it's a directory that images are located\n",
    "    - labels_dir: it's a directory that labels (JSON' files) are located\n",
    "    - image_transform: image transformation function\n",
    "    - mask_transform: mask transformation function\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize directories of images, labels, transformations\n",
    "    def __init__(self, images_dir, labels_dir, image_transform=None, mask_transform=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.image_transform = image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "        self.image_files = sorted(os.listdir(images_dir))\n",
    "        self.label_files = sorted(os.listdir(labels_dir))\n",
    "\n",
    "        # Check whether the name of the image and label are matched or not\n",
    "        assert len(self.image_files) == len(self.label_files), \"Number of image and label files do not match.\"\n",
    "        for img_file, lbl_file in zip(self.image_files, self.label_files):\n",
    "            assert os.path.splitext(img_file)[0] == os.path.splitext(lbl_file)[0], f\"Image and label filename's do not match: {img_file}, {lbl_file}\"\n",
    "\n",
    "    # Return length of the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    # Return the image and mask for the given index\n",
    "    def __getitem__(self, idx):\n",
    "        # Set up paths for image and label files\n",
    "        image_path = os.path.join(self.images_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.labels_dir, self.label_files[idx])\n",
    "\n",
    "        # Load image, then convert it to RGB\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occured while loading your image: {image_path}\\n{e}\")\n",
    "            raise\n",
    "\n",
    "        # Generate a Mask\n",
    "        mask = self.create_mask(label_path, image.size)\n",
    "\n",
    "        # Apply transformation to each image and mask\n",
    "        if self.image_transform is not None:\n",
    "            image = self.image_transform(image)\n",
    "        if self.mask_transform is not None:\n",
    "            mask = self.mask_transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    # Create the mask from label (.JSON) file that has polygons\n",
    "    def create_mask(self, label_path, image_size):\n",
    "        # Create an empty mask image\n",
    "        mask = Image.new('L', image_size, 0)\n",
    "        with open(label_path, 'r') as f:\n",
    "            try:\n",
    "                label_data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Failed to parse JSON file at : {label_path}\")\n",
    "                return mask  # Return empty mask\n",
    "\n",
    "        draw = ImageDraw.Draw(mask)\n",
    "\n",
    "        # Get data(object information) by using 'segmentaion' key value\n",
    "        shapes = label_data.get('segmentation', None)\n",
    "\n",
    "        if shapes is None:\n",
    "            print(f\"There is no data from the label with key of 'segmentation': {label_path}\")\n",
    "            return mask  # Return empty mask\n",
    "\n",
    "        for shape in shapes:\n",
    "            # Bring coordinates of polygon\n",
    "            points = shape.get('polygon', None)\n",
    "            if points is None:\n",
    "                print(f\"객체에 'polygon' 키가 없습니다: {label_path}\")\n",
    "                continue \n",
    "\n",
    "            # Convert the polygon coornidates into integers\n",
    "            points = [tuple(map(int, point)) for point in points]\n",
    "\n",
    "            # Get the label\n",
    "            label = shape.get('name', None)\n",
    "            if label == 'Parking Space':\n",
    "                draw.polygon(points, outline=1, fill=1)\n",
    "            elif label == 'Driveable Space' or label == 'Drivable Space':\n",
    "                draw.polygon(points, outline=2, fill=2)\n",
    "            else:\n",
    "                print(f\"Unknown label '{label}' is found at : {label_path}\")\n",
    "                \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MaskRCNNDataset class\n",
    "class MaskRCNNDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, mask = self.dataset[idx]\n",
    "\n",
    "        # Initialize masks and boxes\n",
    "        masks = []\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        for class_id in [1, 2]:\n",
    "            class_mask = (mask == class_id)\n",
    "            if class_mask.sum() > 0:\n",
    "                num_labels, labels_img = cv2.connectedComponents(\n",
    "                    class_mask.numpy().astype(np.uint8))\n",
    "\n",
    "                for label_id in range(1, num_labels):\n",
    "                    instance_mask = labels_img == label_id\n",
    "                    if instance_mask.sum() > 50:\n",
    "                        instance_mask = torch.from_numpy(instance_mask).float()\n",
    "\n",
    "                        pos = torch.where(instance_mask)\n",
    "                        if len(pos[0]) > 0 and len(pos[1]) > 0:\n",
    "                            xmin = float(pos[1].min())\n",
    "                            xmax = float(pos[1].max())\n",
    "                            ymin = float(pos[0].min())\n",
    "                            ymax = float(pos[0].max())\n",
    "\n",
    "                            if xmax > xmin and ymax > ymin:\n",
    "                                masks.append(instance_mask)\n",
    "                                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                                labels.append(class_id)\n",
    "\n",
    "        # Handle empty predictions\n",
    "        if not masks:\n",
    "            boxes = torch.tensor([[0.0, 0.0, 1.0, 1.0]], dtype=torch.float32)\n",
    "            labels = torch.tensor([0], dtype=torch.int64)\n",
    "            masks = torch.zeros((1, mask.shape[0], mask.shape[1]), dtype=torch.float32)\n",
    "        else:\n",
    "            boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "            labels = torch.tensor(labels, dtype=torch.int64)\n",
    "            masks = torch.stack(masks)\n",
    "\n",
    "        # Normalize the image\n",
    "        if isinstance(image, torch.Tensor) and image.max() > 1.0:\n",
    "            image = image / 255.0\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"masks\": masks\n",
    "        }\n",
    "\n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Transformation and Data Loader (loading data during model training and evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "# Transformations for semantic segmentation\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize the image to 256x256\n",
    "    transforms.ToTensor(),          # Convert the image to a tensor\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256), interpolation=Image.NEAREST),  # Resize the mask with nearest-neighbor interpolation\n",
    "    transforms.PILToTensor(),                                   # Convert the mask to a tensor\n",
    "    transforms.Lambda(lambda x: x.squeeze().long()),            # Remove extra dimensions and convert to long type\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the image for Mask R-CNN (to keep the same image size)\n",
    "maskrcnn_image_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "maskrcnn_mask_transform = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.Lambda(lambda x: x.squeeze().long()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories for each training, testing, and validation \n",
    "train_images_dir = 'seg_dataset/train/images/'\n",
    "train_labels_dir = 'seg_dataset/train/labels/'\n",
    "\n",
    "val_images_dir = 'seg_dataset/validation/images/'\n",
    "val_labels_dir = 'seg_dataset/validation/labels/'\n",
    "\n",
    "test_images_dir = 'seg_dataset/test/images/'\n",
    "test_labels_dir = 'seg_dataset/test/labels/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for semantic segmentaion\n",
    "train_dataset = SegmentationDataset(\n",
    "    train_images_dir, train_labels_dir,\n",
    "    image_transform=image_transform,\n",
    "    mask_transform=mask_transform\n",
    ")\n",
    "\n",
    "val_dataset = SegmentationDataset(\n",
    "    val_images_dir, val_labels_dir,\n",
    "    image_transform=image_transform,\n",
    "    mask_transform=mask_transform\n",
    ")\n",
    "\n",
    "test_dataset = SegmentationDataset(\n",
    "    test_images_dir, test_labels_dir,\n",
    "    image_transform=image_transform,\n",
    "    mask_transform=mask_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for Mask-RCNN\n",
    "maskrcnn_train_dataset = SegmentationDataset(\n",
    "    train_images_dir, train_labels_dir,\n",
    "    image_transform=maskrcnn_image_transform,\n",
    "    mask_transform=maskrcnn_mask_transform\n",
    ")\n",
    "\n",
    "maskrcnn_val_dataset = SegmentationDataset(\n",
    "    val_images_dir, val_labels_dir,\n",
    "    image_transform=maskrcnn_image_transform,\n",
    "    mask_transform=maskrcnn_mask_transform\n",
    ")\n",
    "\n",
    "maskrcnn_test_dataset = SegmentationDataset(\n",
    "    test_images_dir, test_labels_dir,\n",
    "    image_transform=maskrcnn_image_transform,\n",
    "    mask_transform=maskrcnn_mask_transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function of data loader\n",
    "def create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=2):\n",
    "    train_loader = DataLoader(\n",
    "        MaskRCNNDataset(train_dataset),     # get train dateset\n",
    "        batch_size = batch_size,            # set upi batch size for model training\n",
    "        shuffle = True,                     # shuffle them during model training\n",
    "        num_workers = 4,                    # num of worker threads to load dataset\n",
    "        pin_memory = True,                  # for faster GPU data transfer\n",
    "        collate_fn = lambda x: tuple(zip(*x))   # custom collation function to batch data\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        MaskRCNNDataset(val_dataset),\n",
    "        batch_size = 1,\n",
    "        shuffle = False,\n",
    "        num_workers = 4,\n",
    "        pin_memory = True,\n",
    "        collate_fn = lambda x: tuple(zip(*x))\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        MaskRCNNDataset(test_dataset),\n",
    "        batch_size = 1,\n",
    "        shuffle = False,\n",
    "        num_workers = 4,\n",
    "        pin_memory = True,\n",
    "        collate_fn = lambda x: tuple(zip(*x))\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling 'create_dataloaders' in order to generate data loader for Mask RCNN\n",
    "maskrcnn_train_loader, maskrcnn_val_loader, maskrcnn_test_loader = create_dataloaders(\n",
    "    maskrcnn_train_dataset, maskrcnn_val_dataset, maskrcnn_test_dataset, batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualizing Sample Datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset visualization function\n",
    "def visualize_dataset_sample(dataset, num_samples=5):\n",
    "    \"\"\"Visualize samples from the dataset.\"\"\"\n",
    "    indices = random.sample(range(len(dataset)), num_samples)  # Randomly select sample indices\n",
    "\n",
    "    for idx in indices:\n",
    "        image, mask = dataset[idx]\n",
    "        image_np = image.permute(1, 2, 0).numpy()  # Convert image to NumPy array (HWC format)\n",
    "        mask_np = mask.numpy()  # Convert mask to NumPy array\n",
    "\n",
    "        # Colorize the mask\n",
    "        mask_color = np.zeros((mask_np.shape[0], mask_np.shape[1], 3))  # Initialize color mask\n",
    "        mask_color[mask_np == 1] = [0, 1, 0]  # Parking space: Green\n",
    "        mask_color[mask_np == 2] = [0, 0, 1]  # Drivable space: Blue\n",
    "\n",
    "        # Create an overlay\n",
    "        overlay = 0.5 * image_np + 0.5 * mask_color  # Blend image and mask\n",
    "        overlay = np.clip(overlay, 0, 1)  # Ensure values are within [0, 1]\n",
    "\n",
    "        # Visualization\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(image_np)  # Show original image\n",
    "        plt.title('Original')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(mask_np, cmap='gray')  # Show mask\n",
    "        plt.title('Mask')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(overlay)  # Show overlay\n",
    "        plt.title('Overlay')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset samples\n",
    "print(\"Visualizing dataset samples:\")\n",
    "visualize_dataset_sample(train_dataset, num_samples = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask R-CNN model definition\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "def get_maskrcnn_model(num_classes):\n",
    "    # Load a pretrained Mask R-CNN model with a ResNet50 backbone and FPN\n",
    "    model = models.detection.maskrcnn_resnet50_fpn(\n",
    "        pretrained=True,            # Use pretrained weights\n",
    "        box_detections_per_img = 100, # Number of object detections per image\n",
    "        min_size = 800,               # Min image size\n",
    "        max_size = 1333               # Max image size\n",
    "    )\n",
    "\n",
    "    # Modify the box predictor (for bounding boxes)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Mask prediction (for segmentation masks)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256  # size of hidden layer\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
    "        in_features_mask,\n",
    "        hidden_layer,\n",
    "        num_classes\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \"\"\"Calculate an avg and current valuees and Save it\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask R-CNN Training Function\n",
    "def train_maskrcnn(model, dataloaders, optimizer, num_epochs, device):\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=2, factor=0.5, verbose=True\n",
    "    )  # Reduce learning rate\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model_wts = None\n",
    "    history = {'train_loss': [], 'val_loss': []}  # Store losses for each epoch\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nMask R-CNN - Epoch {epoch+1}/{num_epochs}\")\n",
    "        print('-' * 20)\n",
    "\n",
    "        # Iterate through training and validation phases\n",
    "        for phase in ['train', 'val']:\n",
    "            running_loss = 0.0\n",
    "            batch_count = 0\n",
    "\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.train()  # Set model to training mode (fixing BatchNorm issue below)\n",
    "                # Set BatchNorm layers to evaluation mode during validation\n",
    "                for m in model.modules():\n",
    "                    if isinstance(m, torch.nn.BatchNorm2d) or isinstance(m, torch.nn.BatchNorm1d):\n",
    "                        m.eval()\n",
    "\n",
    "            # Loop through batches\n",
    "            for images, targets in tqdm(dataloaders[phase]):\n",
    "                images = [img.to(device) for img in images]  # Move images to the device (GPU)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]  # Move targets to the device\n",
    "\n",
    "                optimizer.zero_grad()  #Clear gradients\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):  # Enable gradients for training\n",
    "                    if phase == 'train':\n",
    "                        # Mixed precision training for faster computation\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            loss_dict = model(images, targets)  # Forward pass\n",
    "                            losses = sum(loss for loss in loss_dict.values())  # Sum all losses\n",
    "\n",
    "                        # Backward pass and optimization\n",
    "                        scaler.scale(losses).backward()\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        # No gradients during validation\n",
    "                        with torch.no_grad():\n",
    "                            loss_dict = model(images, targets)\n",
    "                            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "                    running_loss += losses.item()  # Accumulate loss\n",
    "                    batch_count += 1\n",
    "\n",
    "            epoch_loss = running_loss / (batch_count + 1e-8)  # Calculate average loss for the epoch\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "            history[f'{phase}_loss'].append(epoch_loss)  # Save loss to history\n",
    "\n",
    "            # Save the best model based on validation loss\n",
    "            if phase == 'val':\n",
    "                scheduler.step(epoch_loss)  # Adjust learning rate based on validation loss\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = model.state_dict().copy()  # Store best model weights\n",
    "\n",
    "    print(f'Best val Loss: {best_loss:.4f}')\n",
    "    model.load_state_dict(best_model_wts)  # Load best model weights\n",
    "    return model, history  # Return the best model and the history of losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize the Result of Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot function of training history\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Visualizes the training history.\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title(f'{model_name} - Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # If IoU values are in the history, plot them\n",
    "    if 'train_iou' in history:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['train_iou'], label='Train')\n",
    "        plt.plot(history['val_iou'], label='Validation')\n",
    "        plt.title(f'{model_name} - IoU')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('IoU')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Object Detection & Segmentation at Once\n",
    "### Which is a benefit of using Mask R-CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize Object Detection and Segmentation results simultaneously\n",
    "def visualize_maskrcnn_results(model, dataset, device, num_samples=3, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Visualize both object detection and segmentation results of Mask R-CNN\n",
    "    Args:\n",
    "        model: Mask R-CNN model\n",
    "        dataset: Dataset\n",
    "        device: Execution device (CPU/GPU)\n",
    "        num_samples: Number of samples to visualize\n",
    "        threshold: Object detection confidence threshold\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "\n",
    "    # Define colors for each class\n",
    "    colors = {\n",
    "        1: ('Parking Space', (0, 1, 0)),    # Mask color of parking space = Green\n",
    "        2: ('Driveable Space', (0, 0, 1))   # Msk color of driveable space =  Blue\n",
    "    }\n",
    "\n",
    "    for idx in indices:\n",
    "        image, _ = dataset[idx]\n",
    "        image_input = image.unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(image_input)[0]\n",
    "\n",
    "        # Convert the image to a numpy array\n",
    "        image_np = image.permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Set up the plot for visualizing results\n",
    "        plt.figure(figsize=(15, 5))\n",
    "\n",
    "        # 1. Original Image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(image_np)\n",
    "        plt.title('Original')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # 2. Segmentation Mask\n",
    "        plt.subplot(1, 3, 2)\n",
    "        seg_image = image_np.copy()\n",
    "\n",
    "        # Select only the masks with high confidence\n",
    "        masks = prediction['masks']\n",
    "        scores = prediction['scores']\n",
    "        labels = prediction['labels']\n",
    "\n",
    "        mask_overlay = np.zeros_like(image_np)\n",
    "\n",
    "        for mask, score, label in zip(masks, scores, labels):\n",
    "            if score > threshold:\n",
    "                mask = mask.squeeze().cpu().numpy()\n",
    "                color = colors[label.item()][1]\n",
    "                mask_overlay[mask > 0.5] = color\n",
    "\n",
    "        # Create mask overlay\n",
    "        seg_result = 0.7 * image_np + 0.3 * mask_overlay\n",
    "        seg_result = np.clip(seg_result, 0, 1)\n",
    "        plt.imshow(seg_result)\n",
    "        plt.title('Mask-Overlay')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # 3. Object Detection Results\n",
    "        plt.subplot(1, 3, 3)\n",
    "        det_image = image_np.copy()\n",
    "\n",
    "        # Importing Rectangle from matplotlib for bounding boxes\n",
    "        from matplotlib.patches import Rectangle\n",
    "\n",
    "        det_ax = plt.gca()\n",
    "        det_ax.imshow(det_image)\n",
    "\n",
    "        # Draw bounding boxes\n",
    "        for box, score, label in zip(prediction['boxes'], prediction['scores'], prediction['labels']):\n",
    "            if score > threshold:\n",
    "                box = box.cpu().numpy()\n",
    "                class_name = colors[label.item()][0]\n",
    "                color = colors[label.item()][1]\n",
    "\n",
    "                # Draw the bounding box\n",
    "                rect = Rectangle(\n",
    "                    (box[0], box[1]),\n",
    "                    box[2] - box[0],\n",
    "                    box[3] - box[1],\n",
    "                    linewidth=2,\n",
    "                    edgecolor=color,\n",
    "                    facecolor='none'\n",
    "                )\n",
    "                det_ax.add_patch(rect)\n",
    "\n",
    "                # Add label\n",
    "                det_ax.text(\n",
    "                    box[0],\n",
    "                    box[1] - 5,\n",
    "                    f'{class_name}: {score:.2f}',\n",
    "                    color=color,\n",
    "                    fontsize=8,\n",
    "                    bbox=dict(facecolor='white', alpha=0.5)\n",
    "                )\n",
    "\n",
    "        det_ax.set_title('Detection Results')\n",
    "        det_ax.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Train Mask RCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Mask R-CNN\n",
    "print(\"\\nTraining Mask R-CNN...\")\n",
    "\n",
    "maskrcnn_model = get_maskrcnn_model(num_classes=3)  # 3 classes: background, parking space, driveable space\n",
    "maskrcnn_model = maskrcnn_model.to(device)\n",
    "maskrcnn_optimizer = torch.optim.Adam([                             # Set up diff learning rates for each structure of Mask Rcnn\n",
    "    {'params': maskrcnn_model.backbone.parameters(), 'lr': 1e-4},\n",
    "    {'params': maskrcnn_model.rpn.parameters(), 'lr': 1e-4},\n",
    "    {'params': maskrcnn_model.roi_heads.parameters(), 'lr': 1e-4}\n",
    "])\n",
    "\n",
    "maskrcnn_model, maskrcnn_history = train_maskrcnn(\n",
    "    maskrcnn_model,\n",
    "    {'train': maskrcnn_train_loader, 'val': maskrcnn_val_loader},\n",
    "    maskrcnn_optimizer,\n",
    "    num_epochs=50,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Visualize training curve\n",
    "plot_training_history(maskrcnn_history, 'Mask R-CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Visualize Result of Prediction by the Mask-RCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Mask R-CNN Prediction Results\n",
    "print(\"\\nVisualizing predictions and Parking lot detection results of the Mask R-CNN model:\")\n",
    "visualize_maskrcnn_results(\n",
    "    maskrcnn_model,\n",
    "    maskrcnn_test_dataset,\n",
    "    device,\n",
    "    num_samples = 3,\n",
    "    threshold = 0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualize Mask R-CNN's Detection and Segmentaion Results in a Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "def visualize_combined_results(model, dataset, device, num_samples = 3, threshold = 0.5):\n",
    "    \"\"\"Visualize Mask R-CNN's segmentation and detection results in a single image\"\"\"\n",
    "    model.eval()\n",
    "    indices = random.sample(range(len(dataset)), num_samples)\n",
    "\n",
    "    # Define colors for each class\n",
    "    colors = {\n",
    "        1: ('Parking Space', (0, 1, 0)),    # mask color of parking space = green\n",
    "        2: ('Driveable Space', (0, 0, 1))   # mask color for driveable space = blue\n",
    "    }\n",
    "\n",
    "    for idx in indices:\n",
    "        image, _ = dataset[idx]\n",
    "        image_input = [image.to(device)]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = model(image_input)[0]\n",
    "\n",
    "        # Convert image to numpy array\n",
    "        image_np = image.permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Set up plot for visualization\n",
    "        plt.figure(figsize=(20, 6))\n",
    "\n",
    "        # 1. Original Image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(image_np)\n",
    "        plt.title('Original Image', fontsize = 12, pad = 10)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # 2. Combined result of segmentation mask and bounding boxes\n",
    "        plt.subplot(1, 3, 2)\n",
    "        combined_image = image_np.copy()\n",
    "\n",
    "        # Create mask overlay\n",
    "        masks = prediction['masks']\n",
    "        scores = prediction['scores']\n",
    "        labels = prediction['labels']\n",
    "        boxes = prediction['boxes']\n",
    "\n",
    "        # Apply masks first\n",
    "        mask_overlay = np.zeros_like(image_np)\n",
    "        for mask_, score_, label_ in zip(masks, scores, labels):\n",
    "            if score_ > threshold:\n",
    "                mask_ = mask_.squeeze().cpu().numpy()\n",
    "                color = colors[label_.item()][1]\n",
    "                mask_overlay[mask_ > 0.5] = color\n",
    "\n",
    "        # Combine mask and image\n",
    "        combined_result = 0.7 * combined_image + 0.3 * mask_overlay\n",
    "        combined_result = np.clip(combined_result, 0, 1)\n",
    "        plt.imshow(combined_result)\n",
    "\n",
    "        # Add bounding boxes and labels\n",
    "        ax = plt.gca()\n",
    "        for box, score_, label_ in zip(boxes, scores, labels):\n",
    "            if score_ > threshold:\n",
    "                box = box.cpu().numpy()\n",
    "                class_name = colors[label_.item()][0]\n",
    "                color = colors[label_.item()][1]\n",
    "\n",
    "                # Draw bounding box\n",
    "                rect = patches.Rectangle(\n",
    "                    (box[0], box[1]),\n",
    "                    box[2] - box[0],\n",
    "                    box[3] - box[1],\n",
    "                    linewidth = 2,\n",
    "                    edgecolor = color,\n",
    "                    facecolor = 'none'\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "                # Add label\n",
    "                ax.text(\n",
    "                    box[0],\n",
    "                    box[1] - 5,\n",
    "                    f'{class_name}: {score_:.2f}',\n",
    "                    color = color,\n",
    "                    fontsize = 8,\n",
    "                    bbox = dict(facecolor = 'white', alpha = 0.7)\n",
    "                )\n",
    "\n",
    "        plt.title('Segmentation & Detection Result', fontsize = 12, pad = 10)\n",
    "        plt.axis('off')\n",
    "\n",
    "        # 3. Class-wise Statistics\n",
    "        plt.subplot(1, 3, 3)\n",
    "        class_vis = image_np.copy()\n",
    "        ax = plt.gca()\n",
    "        plt.imshow(class_vis)\n",
    "\n",
    "        # Display each class with a different color\n",
    "        legend_info = []\n",
    "        for label_id, (class_name, color) in colors.items():\n",
    "            mask_combined = np.zeros_like(mask_overlay[:,:,0])\n",
    "            box_count = 0\n",
    "            confidence_sum = 0\n",
    "\n",
    "            for mask_, label_, score_, box in zip(masks, labels, scores, boxes):\n",
    "                if score_ > threshold and label_.item() == label_id:\n",
    "                    mask_ = mask_.squeeze().cpu().numpy()\n",
    "                    mask_combined = np.logical_or(mask_combined, mask_ > 0.5)\n",
    "                    box = box.cpu().numpy()\n",
    "                    box_count += 1\n",
    "                    confidence_sum += score_.item()\n",
    "\n",
    "                    # Draw bounding box\n",
    "                    rect = patches.Rectangle(\n",
    "                        (box[0], box[1]),\n",
    "                        box[2] - box[0],\n",
    "                        box[3] - box[1],\n",
    "                        linewidth = 2,\n",
    "                        edgecolor = color,\n",
    "                        facecolor = 'none'\n",
    "                    )\n",
    "                    ax.add_patch(rect)\n",
    "\n",
    "            if box_count > 0:\n",
    "                avg_confidence = confidence_sum / box_count\n",
    "                legend_info.append(f'{class_name}: {box_count} objects (avg conf: {avg_confidence:.2f})')\n",
    "\n",
    "        # Add legend\n",
    "        ax.text(\n",
    "            10, 20,\n",
    "            '\\n'.join(legend_info),\n",
    "            fontsize = 10,\n",
    "            bbox=dict(facecolor = 'white', alpha = 0.7)\n",
    "        )\n",
    "\n",
    "        plt.title('Class-wise Detection Statistics', fontsize = 12, pad = 10)\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print detected object count and average confidence\n",
    "        print(f\"\\nImage {idx} Detection Statistics:\")\n",
    "        class_stats = {}\n",
    "        for label_id, (class_name, _) in colors.items():\n",
    "            class_scores = [score.item() for score, label in zip(scores, labels)\n",
    "            if label.item() == label_id and score.item() > threshold]\n",
    "            if class_scores:\n",
    "                avg_conf = sum(class_scores) / len(class_scores)\n",
    "                class_stats[class_name] = {\n",
    "                    'count': len(class_scores),\n",
    "                    'avg_confidence': avg_conf\n",
    "                }\n",
    "                print(f\"{class_name}: {len(class_scores)} objects, Average confidence: {avg_conf:.3f}\")\n",
    "\n",
    "# Visualizing Mask R-CNN results\n",
    "print(\"\\nVisualizing combined Mask R-CNN results...\")\n",
    "visualize_combined_results(\n",
    "    maskrcnn_model,\n",
    "    test_dataset,\n",
    "    device,\n",
    "    num_samples = 3,\n",
    "    threshold = 0.5\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
